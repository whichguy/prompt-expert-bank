{
  "systemContext": "============================================================\nREPOSITORY CONTEXT\n============================================================\n\nRepository: context-test-demo\nFiles Loaded: 3 text files\nGit Branch: test-full-context-evaluation (d2c7ba4)\n\nFILES:\n------------------------------------------------------------\n\nFILE: README.md [Hash: aaffc88c]\n----------------------------------------\n# Test Context Demo\n\nThis is a test repository for demonstrating context-aware evaluation.\n\n## Key Components\n\n- **API Configuration**: See `config/api-config.json` for endpoint definitions\n- **Data Processing**: The `data/processor.js` handles batch processing with caching\n- **Rate Limits**: 60 requests per minute as configured\n- **Authentication**: Uses `/api/v2/auth/login` endpoint\n\n\nFILE: config/api-config.json [Hash: 533f70d0]\n----------------------------------------\n{\n  \"endpoints\": {\n    \"auth\": \"/api/v2/auth/login\",\n    \"users\": \"/api/v2/users\",\n    \"data\": \"/api/v2/data\"\n  },\n  \"rateLimit\": {\n    \"requests\": 60,\n    \"window\": \"1m\"\n  },\n  \"timeout\": 5000\n}\n\n\nFILE: data/processor.js [Hash: ac2b91ed]\n----------------------------------------\nconst API_KEY = 'UNIQUE_PROCESSING_KEY_789';\n\nclass DataProcessor {\n  constructor() {\n    this.maxBatchSize = 1000;\n    this.cache = new Map();\n  }\n  \n  async processData(data) {\n    // Validate input\n    if (!data || !Array.isArray(data)) {\n      throw new Error('Invalid data format');\n    }\n    \n    // Process in batches\n    const results = [];\n    for (let i = 0; i < data.length; i += this.maxBatchSize) {\n      const batch = data.slice(i, i + this.maxBatchSize);\n      const processed = await this.processBatch(batch);\n      results.push(...processed);\n    }\n    \n    return results;\n  }\n  \n  async processBatch(batch) {\n    return batch.map(item => ({\n      ...item,\n      processed: true,\n      timestamp: Date.now(),\n      key: API_KEY\n    }));\n  }\n}\n\nmodule.exports = DataProcessor;\n\n============================================================\nEND REPOSITORY CONTEXT\n============================================================",
  "textFiles": [
    {
      "hash": "aaffc88c4a90f441eb26683fff67616c30281140",
      "paths": [
        "README.md"
      ],
      "content": "# Test Context Demo\n\nThis is a test repository for demonstrating context-aware evaluation.\n\n## Key Components\n\n- **API Configuration**: See `config/api-config.json` for endpoint definitions\n- **Data Processing**: The `data/processor.js` handles batch processing with caching\n- **Rate Limits**: 60 requests per minute as configured\n- **Authentication**: Uses `/api/v2/auth/login` endpoint",
      "type": "text"
    },
    {
      "hash": "533f70d0dba1ca0734c7b01092ce64d45b208c55",
      "paths": [
        "config/api-config.json"
      ],
      "content": "{\n  \"endpoints\": {\n    \"auth\": \"/api/v2/auth/login\",\n    \"users\": \"/api/v2/users\",\n    \"data\": \"/api/v2/data\"\n  },\n  \"rateLimit\": {\n    \"requests\": 60,\n    \"window\": \"1m\"\n  },\n  \"timeout\": 5000\n}",
      "type": "text"
    },
    {
      "hash": "ac2b91ed59585d7f058d3e378712b27c6a4c2d3c",
      "paths": [
        "data/processor.js"
      ],
      "content": "const API_KEY = 'UNIQUE_PROCESSING_KEY_789';\n\nclass DataProcessor {\n  constructor() {\n    this.maxBatchSize = 1000;\n    this.cache = new Map();\n  }\n  \n  async processData(data) {\n    // Validate input\n    if (!data || !Array.isArray(data)) {\n      throw new Error('Invalid data format');\n    }\n    \n    // Process in batches\n    const results = [];\n    for (let i = 0; i < data.length; i += this.maxBatchSize) {\n      const batch = data.slice(i, i + this.maxBatchSize);\n      const processed = await this.processBatch(batch);\n      results.push(...processed);\n    }\n    \n    return results;\n  }\n  \n  async processBatch(batch) {\n    return batch.map(item => ({\n      ...item,\n      processed: true,\n      timestamp: Date.now(),\n      key: API_KEY\n    }));\n  }\n}\n\nmodule.exports = DataProcessor;",
      "type": "text"
    }
  ],
  "mediaFiles": [],
  "cacheControl": {
    "cacheKeys": [
      "aaffc88c4a90f441eb26683fff67616c30281140",
      "533f70d0dba1ca0734c7b01092ce64d45b208c55",
      "ac2b91ed59585d7f058d3e378712b27c6a4c2d3c"
    ],
    "ttl": 3600
  },
  "summary": {
    "totalFiles": 3,
    "textFiles": 3,
    "imageFiles": 0,
    "pdfFiles": 0,
    "uniqueHashes": 3,
    "totalSize": 1375
  },
  "files": [
    {
      "hash": "aaffc88c4a90f441eb26683fff67616c30281140",
      "path": "README.md",
      "type": "text",
      "size": 387,
      "content": "# Test Context Demo\n\nThis is a test repository for demonstrating context-aware evaluation.\n\n## Key Components\n\n- **API Configuration**: See `config/api-config.json` for endpoint definitions\n- **Data Processing**: The `data/processor.js` handles batch processing with caching\n- **Rate Limits**: 60 requests per minute as configured\n- **Authentication**: Uses `/api/v2/auth/login` endpoint"
    },
    {
      "hash": "533f70d0dba1ca0734c7b01092ce64d45b208c55",
      "path": "config/api-config.json",
      "type": "text",
      "size": 195,
      "content": "{\n  \"endpoints\": {\n    \"auth\": \"/api/v2/auth/login\",\n    \"users\": \"/api/v2/users\",\n    \"data\": \"/api/v2/data\"\n  },\n  \"rateLimit\": {\n    \"requests\": 60,\n    \"window\": \"1m\"\n  },\n  \"timeout\": 5000\n}"
    },
    {
      "hash": "ac2b91ed59585d7f058d3e378712b27c6a4c2d3c",
      "path": "data/processor.js",
      "type": "text",
      "size": 793,
      "content": "const API_KEY = 'UNIQUE_PROCESSING_KEY_789';\n\nclass DataProcessor {\n  constructor() {\n    this.maxBatchSize = 1000;\n    this.cache = new Map();\n  }\n  \n  async processData(data) {\n    // Validate input\n    if (!data || !Array.isArray(data)) {\n      throw new Error('Invalid data format');\n    }\n    \n    // Process in batches\n    const results = [];\n    for (let i = 0; i < data.length; i += this.maxBatchSize) {\n      const batch = data.slice(i, i + this.maxBatchSize);\n      const processed = await this.processBatch(batch);\n      results.push(...processed);\n    }\n    \n    return results;\n  }\n  \n  async processBatch(batch) {\n    return batch.map(item => ({\n      ...item,\n      processed: true,\n      timestamp: Date.now(),\n      key: API_KEY\n    }));\n  }\n}\n\nmodule.exports = DataProcessor;"
    }
  ]
}
